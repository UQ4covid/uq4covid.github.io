\documentclass[a4paper]{article}
\usepackage{amsmath, bm, xcolor, tikz, algpseudocode, algorithm, natbib, mathtools}
\usetikzlibrary{arrows, positioning, backgrounds, fit}

% formatting
\usepackage[margin=1in]{geometry}
\setlength{\parskip}{\baselineskip}
\setlength{\parindent}{0pt}

\title{Data Assimilation and Calibration for large spatio-temporal ID models}
\author{TJ McKinley and Danny Williamson}
\date{}

% custom commands for brevity
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bX}{\bm{X}}
\newcommand{\by}{\bm{y}}
\newcommand{\bp}{\bm{p}}
\newcommand{\br}{\bm{r}}
\newcommand{\bY}{\bm{Y}}
\newcommand{\bZ}{\bm{Z}}
\newcommand{\bU}{\bm{U}}
\newcommand{\bD}{\bm{D}}
\newcommand{\gap}[1][1]{\vspace{#1\baselineskip}}
\newcommand{\srm}[1]{\mbox{\scriptsize #1}}

\bibliographystyle{asa}

\begin{document}

\maketitle

\subsection*{Overview}
We have a slow spatially explicit meta population Covid-19 model, and observations of deaths (and later cases) we may use to calibrate its parameters. Early in the pandemic, the parameters and the initial states are both extremely important and unknown. Whilst there are $O(10)$ parameters, treating the unknown initial states makes calibration an $O(10^5)$ parameter problem that cannot be solved by traditional UQ methods with a model of this complexity. Our intention is to explore a hybrid data assimilation/UQ calibration approach to nudge the initial states and to calibrate the model parameters.

\subsection*{Statistical modelling}
Let $\bY_{st}$ denote the true Covid-19 state vector at location $s$ and time $t$. This vector includes numbers in the E, A, I and H classes as well as the number of deaths, both in hospitals and the community. We have underreported observations $\bZ_t$ relevant to some part of this state vector, indicated by incidence matrix H. To capture underreporting let 
\begin{equation}\label{obs}
\bZ_{st} = H\bY_{st} - \bU_{st}(Y_{st}),
\end{equation}
where $\bU_{st}(Y_{st})$ is an underreporting process. Our Covid-19 model has parameters $\btheta$ and a single run generates $\bX_{st}(\btheta)$ for all $s$ and $t$ of interest. Letting $i$ index the output class (e.g. asymptomatics), we model the true Covid-19 state via
\begin{equation}\label{best}
Y_{sti} = X_{sti}(\btheta^*) + D_{sti}(X_{sti}),
\end{equation}
where $\btheta^*$ is a particular ``best input'' for the calibration parameters and $D$ is a count discrepancy that depends on the model state (to allow for the fact that the higher the count, the larger the discrepancy between the actual count and the model count can be whilst retaining a good model). We should view $\bD(\bX)$ as a tolerance to error process rather than a true discrepancy (a nuance important for history matching and interpretation later).

A model for the error processes is
\begin{align}
\bU_{st}\mid\bY_{st} &\sim \mathrm{Poisson}(\mu_s(\bY_{st})) \\
D_{sti}\mid X_{sti} &\sim \mathrm{tSkellam}(\lambda_{si}(X_{sti}), \lambda_{si}(X_{sti}), -X_{sti}),
\end{align}
where the tSkellam distribution is a Skellam distribution truncated below at its third argument. Specifically, let $K \sim \mathrm{Skellam}(m_1, m_2)$ then $K' \sim \mathrm{tSkellam}(m_1, m_2, \tau)$ has $P(K' < \tau) = 0$ and $P(K' = k) = P(K=k)/P(K>\tau)$.

The functions $\mu$ and $\lambda$ are chosen a priori (though some parameters may be unknown and folded into the calibration later) to reflect known spatio-temporal issues with underreporting as well as a principle that the more cases there are, the more we expect to be missed ($\mu$), and to control the standard deviation of $D$ at different levels of $X_{sti}$ (using the property of the untruncated Skellam distribution that the variance is $m_1+m_2$ and the mean is $m_1-m_2$).

\section*{Calibration mode}
During calibration mode, we assume that the initial states of every simulation $X_{t-p:t}(\btheta)$, ($X_{t-p}(\theta)$) are ``compatible" with $\bZ_{1:t-p}$ under $\theta=\theta^*$. We might phrase this as the initial states are a draw from $\pi(X_{t-p} \mid \btheta=\btheta^*, \bZ_{1:t-p})$, and that is probably the final goal, but ``compatible" can mean ``near enough" given the HM steps. During this phase we use a history matching approach to rule out values of $\btheta$ as being implausible (specifically meaning that they could not be $\btheta^*$) through (\ref{best}) and (\ref{obs}), using any subset of $\bZ_{t-p:t}$. The important thing is not to use data that have been used in drawing the initial states. There are at least 4 approaches to this calibration we could explore: 
\begin{enumerate}
\item Emulate and HM to a collection of individual counts. Perhaps this is the most obvious way to proceed and would require a "count implausibility" measure (no such measure currently exists because distance in this discrete subset of the integers is not an obvious concept). We might use something along the lines of $P(\bZ_{st} - \by_{st}| > T) \approx 0$ as an implausibility type rule. The key point is that this version requires emulators of all elements of the model that go into the measure (a many outputs approach to HM). Currently our emulators for this approach are
\begin{align*}
X_{sti}(\btheta) &\sim \mathrm{NB}(\phi(\btheta), \psi(\btheta)) \\
\phi(\btheta) &\sim \mathrm{DGP}(0, k_1(\cdot, \cdot)) \\
\psi(\btheta) &\sim \mathrm{DGP}(0, k_2(\cdot, \cdot))
\end{align*}
\item Construct complex spatio-temporal emulation of $\bX(\btheta)$ based on dimension reduction (kPCA as Wenzhe did, Poisson PCA that James found, something else). Means fewer emulators, a model of the model (always good for finding structures) and can use the same implausibility developed in 1.
\item Once the implausibility is derived in 1, remove the emulator part of it (so true implausibility with the model embedded) and then emulate the implausibility. Then cut away regions based on the implausibility emulator.
\item Similar to 3, but emulate the likelihood and kill near 0 regions. This is tricky to think about (as an implausibility over the likelihood needs to be constructed and justified), but has been published (Youngman and Oakley). %One advantage might be that you can generate an unbiased estimate of the likelihood directly from the PF, which doesn't involve any reduction of the data and for which (in theory) an estimate of the variance of the estimator could also be constructed. This would also automatically allow for data assimilation through the bootstrap resampling, but would only require one emulator to be constructed and would do DA and calibration in one step potentially.
\end{enumerate}

\section*{Data assimilation mode}
Here, for each parameter set of a design $\btheta_1, \ldots, \btheta_n$, we want to draw from $\pi(X_{t} \mid \btheta_i=\btheta^*, \bZ_{t-p:t})$. I think this involves integrating $\bY$ and not-required $X_i$ from 
\begin{equation}\label{dwDA}
    \pi\left(\bZ_{t-p:t}, \bY_{t-p:t}, \bX_{t-p:t}\right) = \pi\left(X_{t-p}\right)\pi\left(Y_{t-p} \mid X_{t-p}\right)\prod_{j=p+1}^0 \pi\left(Z_{t-j} \mid Y_{t-j}\right)\pi\left(Y_{t-j} \mid X_{t-j}\right)\pi\left(X_{t-j} \mid Y_{t-j + 1}\right).
\end{equation}

$\pi\left(Z_j \mid Y_j\right)$ is available through (\ref{obs}), $\pi\left(Y_j \mid X_j\right)$ through (\ref{best}), both in terms of the density (if we wanted to evaluate (\ref{dwDA}) as a likelihood) or for sampling. $\pi\left(X_j \mid Y_{j - 1}\right)$ is a 1 step call to Metawards and, whilst we might insert a GP here, such a dynamic emulator requires the full MW state vector as input. As such, it is likely that we can only sample from this density (by running MW). 

A Data assimilation targets (\ref{dwDA}). The easiest appropriate method to follow is the Bootstrap Particle Filter (BPF), which follows the following pseudo algorithm \textit{independently for every $\btheta$}:
\begin{enumerate}
\item Start at $X_{t-p-1}$ and draw $M$ particles from $\pi(X_{t-p}\mid X_{t-p-1})$.
\item For each particle $X^i_{t-p}, (i=1, \ldots, M)$, sample from $\pi\left(Y_{t-p} \mid X^i_{t-p}\right)$ under (\ref{best}) (see discrepancy section on Skellam constraints to be written later).
\item Weight all particles according to $\pi\left(Z_{t-p} \mid Y^{i}_{t-p}\right)$ and then resample the particles $Y^i_{t-p}$ according to these weights.
\item For j = p + 1, \ldots, 0, 
\begin{enumerate}
\item By running Metawards directly for a single time step, for $i = 1, \ldots, M$ sample $\pi\left(X_{t-j} \mid Y^i_{t- j + 1}\right)$.
\item Use these samples to generate a particle from each $\pi\left(Y_{t-j} \mid X^i_{t-j}\right)$.
\item Weight the $M$ particles according to $\pi\left(Z_{t-j} \mid Y^{i}_{t-j}\right)$ and then resample the $Y^i_{t-j}$ according to these weights.
\end{enumerate}
\end{enumerate}

The final state $Y_t$ or even a previous state can be used as a restart point for running MW in calibration mode via any of the 4 methods mentioned previously. Note any other type of particle filter could be used for the above (essentially amounting to different ways of selecting particles according to the weights in step 3 and 4c.

\section*{A Hybrid DA-Calibration mode}
This idea is really calibration mode option 4 using extra theory to do everything in 1 step. For option 4, we use (\ref{best}) and (\ref{obs}) to form a likelihood from our simulations at any $\btheta$, and then emulate that likelihood and use it for calibration. For example, under a uniform prior, you can treat this as an emulation of the posterior as a function of $\btheta$ and then map out the high density or 0 density regions as desired. 

Once a DA (such as the BPF) has run over a given time window, the product (over the timesteps) of the average weights (averaged over the $M$ particles at that time) is an unbiased estimate of (\ref{dwDA}) for the chosen time window for each $\btheta$. As such, we can view the DA as the model with the relatively small number of particles inducing noise that may depend on $\btheta$. Theoretically, it is then straightforward to emulate the likelihood (accounting for this additional Monte Carlo type noise via a nugget or stochastic emulator (such as a DGP) as desired. We can explore the sensitivity of the likelihood to the parameters and we can attempt a calibration or a HM based on level sets of the likelihood as we want. 

Our proposal (DW and TJ) is that we make a serious attempt at this hybrid method over January. TJ has already got a version of the BPF working and we've been working this week on sampling from $P(Y|X)$ using truncated Skellam distributions. This is complicated by enforcing certain epidemiological constraints across the model classes (e.g. you can't have more people die than have been exposed etc), but the details of this are now sorted. 

The main difficulty now is building in single step iterations of MetaWards with a chosen starting state vector efficiently. The MW code does not currently allow for this to be quick at all and it probably never will, so we might be at the mercy of Catalyst at ward level. TJ is going to provide us an RCPP non-spatial version of MW this week. We are then planning a LAD version of MW, probably via the RCPP route that we can run on our own machines. For those of us with a UQ background, the point is to put the model (the likelihood estimates described above) in our hands so that we can figure out emulation/SA/HM etc with it. 

\section*{Model discrepancy constraints}

\textbf{NOTE: WE USE $D()$ TO DENOTE DISCREPANCY ABOVE, BUT WE ALSO USE THIS IN THE MODEL TO CORRESPOND TO DEATH COUNTS, SO I AM USING $M_{state}$ TO DENOTE MODEL DISCREPANCY HERE. THIS IS A FLAG TO COME BACK TO NOTATION LATER.}

Some justification and detail of the model discrepancy process below.

For \textbf{absorbing states} ($D_H$, $R_H$, $D_I$, $R_I$ and $R_A$) we place the model discrepancy on the \textit{incidence} (i.e. new cases) rather than the state counts. For all other states, we place the model discrepancy on the numbers of individuals in each state directly. A key aspect is that we must ensure that any model discrepancy added results in states that are \textit{epidemiologically} valid. Hence we can't have more deaths than infections and such like. This means we add constraints onto the model discrepancy process based on the structure of the model.

Because we're focusing on individual states, we will use the notation e.g. $D^X_{Ht}$ to denote the state counts from the \textit{simulator} at time $t$ (relating to $X_t$ above), $D^Y_{Ht}$ is the \textit{adjusted} state after model discrepancy is added, and e.g. $D^{X^\prime}_{Ht}$ is the \textit{incidence}.

Thinking first about $D^Y_{Ht}$ we have that $D^Y_{Ht} = D^Y_{H(t - 1)} + D^{Y^\prime}_{Ht}$, and hence we can derive bounds:
\begin{equation*}
    0 \leq D^{Y^\prime}_{Ht} \leq H^Y_{t-1} \qquad \mbox{since $D^{Y^\prime}_{Ht} \leq H^Y_{t - 1}$}.
\end{equation*}
We let the adjusted incidence $D^{Y^\prime}_{Ht} = D^{X^\prime}_{Ht} + M^\prime_{D_Ht}$, and then place a truncated Skellam distribution on $M^\prime_{D_Ht}$, with constraints:
\begin{equation*}
    -D^{X^\prime}_{Ht} \leq M^\prime_{D_Ht} \leq H^Y_{t - 1} - D^{X^\prime}_{Ht}.
\end{equation*}
Then the adjusted count is given by
\begin{equation*}
    D^Y_{Ht} = D^X_{Ht} + M^\prime_{D_Ht}.
\end{equation*}
This holds since
\begin{eqnarray*}
    D^Y_{Ht} &=& D^Y_{H(t - 1)} + D^{Y^\prime}_{Ht}\\
    &=& D^Y_{H(t - 1)} + D^{X^\prime}_{Ht} + M^\prime_{D_Ht}\\
    &=& D^X_{Ht} + M^\prime_{D_Ht} \qquad \mbox{since}~D^X_{Ht} = D^Y_{H(t - 1)} + D^{X^\prime}_{Ht}.
\end{eqnarray*}
To summarise:
\begin{align*}
\Aboxed{M^\prime_{D_Ht} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-D^{X^\prime}_{Ht}, H^Y_{t - 1} - D^{X^\prime}_{Ht}\right)}\\
    \Aboxed{D^{Y^\prime}_{Ht} &= D^{X^\prime}_{Ht} + M^\prime_{D_Ht}}\\
    \Aboxed{D^Y_{Ht} &= D^X_{Ht} + M^\prime_{D_Ht}}
\end{align*}

We also know that
\begin{align*}
    &D^{Y^\prime}_{Ht} + R^{Y^\prime}_{Ht} \leq H^Y_{t - 1}\\
    \Rightarrow 0 &\leq R^{Y^\prime}_{Ht} \leq H^Y_{t - 1} - D^{Y^\prime}_{Ht},
\end{align*}
and hence we can derive a constraint for $M^\prime_{R_Ht}$ conditional on the adjusted $D^{Y^\prime}_{Ht}$, such that
\begin{equation*}
    -R^{X^\prime}_{Ht} \leq M^\prime_{R_Ht} \leq H^Y_{t - 1} - D^{Y^\prime}_{Ht} - R^{X^\prime}_{Ht}.
\end{equation*}
Hence we have:
\begin{align*}
\Aboxed{M^\prime_{R_Ht} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-R^{X^\prime}_{Ht}, H^Y_{t - 1} - D^{Y^\prime}_{Ht} - R^{X^\prime}_{Ht}\right)}\\
    \Aboxed{R^{Y^\prime}_{Ht} &= R^{X^\prime}_{Ht} + M^\prime_{R_Ht}}\\
    \Aboxed{R^Y_{Ht} &= R^X_{Ht} + M^\prime_{R_Ht}}
\end{align*}

Starting initially with $H^Y_t$, we then know that
\begin{equation*}
    H^Y_t = H^Y_{t - 1} + H^{Y^\prime}_t - D^{Y^\prime}_{Ht} - R^{Y^\prime}_{Ht},
\end{equation*}
and we now place the model discrepancy on the state and \textit{not} the incidence, since $H^Y_t$ is not an absorbing state. Therefore, 
\begin{equation*}
    H^Y_t = H^X_t + M_{Ht}.
\end{equation*}
Hence
\begin{align*}
    H^Y_t &= H^Y_{t - 1} + H^{Y^\prime}_t - D^{Y^\prime}_{Ht} - R^{Y^\prime}_{Ht}\\
    \Rightarrow H^{Y^\prime}_t &= H^Y_t - H^Y_{t - 1} + D^{Y^\prime}_{Ht} + R^{Y^\prime}_{Ht}\\
    \Rightarrow H^{Y^\prime}_t &= H^X_t + M_{Ht} - H^Y_{t - 1} + D^{Y^\prime}_{Ht} + R^{Y^\prime}_{Ht}.
\end{align*}
Since $0 \leq H^{Y^\prime}_t \leq I^Y_{1(t - 1)}$ we can thus derive bounds:
\begin{align*}
    0 &\leq H^X_t + M_{Ht} - H^Y_{t - 1} + D^{Y^\prime}_{Ht} + R^{Y^\prime}_{Ht} \leq I^Y_{1(t - 1)}\\
    \Rightarrow -H^X_t + H^Y_{t - 1} - D^{Y^\prime}_{Ht} - R^{Y^\prime}_{Ht} &\leq M_{Ht} \leq I^Y_{1(t - 1)} -H^X_t + H^Y_{t - 1} - D^{Y^\prime}_{Ht} - R^{Y^\prime}_{Ht}.
\end{align*}
Hence
\begin{align*}
\Aboxed{M_{Ht} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-H^X_t + H^Y_{t - 1} - D^{Y^\prime}_{Ht} - R^{Y^\prime}_{Ht}, I^Y_{1(t - 1)} -H^X_t + H^Y_{t - 1} - D^{Y^\prime}_{Ht} - R^{Y^\prime}_{Ht}\right)}\\
    \Aboxed{H^Y_{t} &= H^X_{t} + M_{Ht}}\\
    \Aboxed{H^{Y^\prime}_{t} &= H^X_t + M_{Ht} - H^Y_{t - 1} + D^{Y^\prime}_{Ht} + R^{Y^\prime}_{Ht}}
\end{align*}

Now consider $D^Y_{It}$. Since this is an absorbing state, we place MD on the \textit{incidence} $D^{Y^\prime}_{It}$. We have that
\begin{equation*}
    D^{Y^\prime}_{It} + H^{Y^\prime}_t \leq I^Y_{1(t - 1)}
\end{equation*}
and $D^{Y^\prime}_{It} \geq 0$, and hence we can derive bounds for the MD as:
\begin{align*}
    0 &\leq D^{Y^\prime}_{It} \leq I^Y_{1(t - 1)} - H^{Y^\prime}_t\\
    0 &\leq D^{X^\prime}_{It} + M^\prime_{D_It} \leq I^Y_{1(t - 1)} - H^{Y^\prime}_t\\ 
    -D^{X^\prime}_{It} &\leq M^\prime_{D_It} \leq I^Y_{1(t - 1)} - H^{Y^\prime}_t - D^{X^\prime}_{It}.
\end{align*}
Hence
\begin{align*}
\Aboxed{M^\prime_{D_It} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-D^{X^\prime}_{It}, I^Y_{1(t - 1)} - H^{Y^\prime}_{t} - D^{X^\prime}_{It}\right)}\\
    \Aboxed{D^{Y^\prime}_{It} &= D^{X^\prime}_{It} + M^\prime_{D_It}}\\
    \Aboxed{D^Y_{It} &= D^X_{It} + M^\prime_{D_It}}
\end{align*}

We also have that $0 \leq R^{Y^\prime}_{It} \leq I^{Y^\prime}_{2(t - 1)}$, leading to:
\begin{align*}
\Aboxed{M^\prime_{R_It} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-R^{X^\prime}_{It}, I^Y_{2(t - 1)} - R^{X^\prime}_{It}\right)}\\
    \Aboxed{R^{Y^\prime}_{It} &= R^{X^\prime}_{It} + M^\prime_{R_It}}\\
    \Aboxed{R^Y_{It} &= R^X_{It} + M^\prime_{R_It}}
\end{align*}

Next, we know that
\begin{equation*}
    I^Y_{2t} = I^Y_{2(t - 1)} + I^{Y^\prime}_{2t} - R^{Y^\prime}_{It},
\end{equation*}
and we place MD on the \textit{counts}, such that:
\begin{equation*}
    I^Y_{2t} = I^X_{2t} + M_{I_2t}
\end{equation*}
We know that:
\begin{align*}
    I^Y_{2t} &= I^Y_{2(t - 1)} + I^{Y^\prime}_{2t} - R^{Y^\prime}_{It}\\
    \Rightarrow I^{Y^\prime}_{2t} &= I^Y_{2t} - I^Y_{2(t - 1)} + R^{Y^\prime}_{It}\\
    \Rightarrow I^{Y^\prime}_{2t} &= I^X_{2t} + M_{I_2t} - I^Y_{2(t - 1)} + R^{Y^\prime}_{It}
\end{align*}
Since $0 \leq I^{Y^\prime}_{2t} \leq I^Y_{1(t - 1)} - D^{Y^\prime}_{It} - H^{Y^\prime}_t$, we can derive bounds:
\begin{align*}
    0 &\leq I^{Y^\prime}_{2t} \leq I^Y_{1(t - 1)} - D^{Y^\prime}_{It} - H^{Y^\prime}_t\\
    \Rightarrow 0 &\leq I^X_{2t} + M_{I_2t} - I^Y_{2(t - 1)} + R^{Y^\prime}_{It} \leq I^Y_{1(t - 1)} - D^{Y^\prime}_{It} - H^{Y^\prime}_t\\
    \Rightarrow -I^X_{2t} + I^Y_{2(t - 1)} - R^{Y^\prime}_{It} &\leq M_{I_2t} \leq I^Y_{1(t - 1)} - D^{Y^\prime}_{It} - H^{Y^\prime}_t - I^X_{2t} + I^Y_{2(t - 1)} - R^{Y^\prime}_{It}.
\end{align*}
Hence
\begin{align*}
\Aboxed{M_{I_2t} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-I^X_{2t} + I^Y_{2(t - 1)} - R^{Y^\prime}_{It}, I^Y_{1(t - 1)} - D^{Y^\prime}_{It} - H^{Y^\prime}_t - I^X_{2t} + I^Y_{2(t - 1)} - R^{Y^\prime}_{It}\right)}\\
    \Aboxed{I^Y_{2t} &= I^X_{2t} + M_{I_2t}}\\
    \Aboxed{I^{Y^\prime}_{2t} &= I^X_{2t} + M_{I_2t} - I^Y_{2(t - 1)} + R^{Y^\prime}_{It}}
\end{align*}

Then we can construct $I^Y_{1t}$ by considering
\begin{equation*}
    I^Y_{1t} = I^Y_{1(t - 1)} + I^{Y^\prime}_{1t} - I^{Y^\prime}_{2t} - D^{Y^\prime}_{It} - H^{Y^\prime}_t,
\end{equation*}
and we place MD on the \textit{counts}, such that:
\begin{equation*}
    I^Y_{1t} = I^X_{1t} + M_{I_1t}
\end{equation*}
Hence we have:
\begin{align*}
    I^Y_{1t} &= I^Y_{1(t - 1)} + I^{Y^\prime}_{1t} - I^{Y^\prime}_{2t} - D^{Y^\prime}_{It} - H^{Y^\prime}_t\\
    I^{Y^\prime}_{1t} &= I^Y_{1t} - I^Y_{1(t - 1)} + I^{Y^\prime}_{2t} + D^{Y^\prime}_{It} + H^{Y^\prime}_t
\end{align*}
and since $0 \leq I^{Y^\prime}_{1t} \leq P^Y_{t - 1}$ we can derive bounds:
\begin{align*}
    0 &\leq I^{Y^\prime}_{1t} \leq P^Y_{t - 1}\\
    0 &\leq I^Y_{1t} - I^Y_{1(t - 1)} + I^{Y^\prime}_{2t} + D^{Y^\prime}_{It} + H^{Y^\prime}_t \leq P^Y_{t - 1}\\
    0 &\leq I^X_{1t} + M_{I_1t} - I^Y_{1(t - 1)} + I^{Y^\prime}_{2t} + D^{Y^\prime}_{It} + H^{Y^\prime}_t \leq P^Y_{t - 1}\\
    -I^X_{1t} + I^Y_{1(t - 1)} - I^{Y^\prime}_{2t} - D^{Y^\prime}_{It} - H^{Y^\prime}_t &\leq M_{I_1t} \leq P^Y_{t - 1} - I^X_{1t} + I^Y_{1(t - 1)} - I^{Y^\prime}_{2t} - D^{Y^\prime}_{It} - H^{Y^\prime}_t
\end{align*}
Hence
\begin{align*}
\Aboxed{M_{I_1t} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-I^X_{1t} + I^Y_{1(t - 1)} - I^{Y^\prime}_{2t} - D^{Y^\prime}_{It} - H^{Y^\prime}_t, P^Y_{t - 1} - I^X_{1t} + I^Y_{1(t - 1)} - I^{Y^\prime}_{2t} - D^{Y^\prime}_{It} - H^{Y^\prime}_t\right)}\\
    \Aboxed{I^Y_{1t} &= I^X_{1t} + M_{I_1t}}\\
    \Aboxed{I^{Y^\prime}_{1t} &= I^X_{1t} + M_{I_1t} - I^Y_{1(t - 1)} + I^{Y^\prime}_{2t} + D^{Y^\prime}_{It} + H^{Y^\prime}_t}
\end{align*}

For $P^Y_t$ we can consider that
\begin{equation*}
    P^Y_t = P^Y_{t - 1} + P^{Y^\prime}_t - I^{Y^\prime}_{1t}
\end{equation*}
with MD placed on the \textit{counts} as
\begin{equation*}
    P^Y_t = P^X_T + M_{Pt}.
\end{equation*}
Since $0 \leq P^{Y^\prime}_t \leq E^Y_{t - 1}$, we can derive bounds
\begin{align*}
    0 &\leq P^{Y^\prime}_t \leq E^Y_{t - 1}\\
    0 &\leq P^Y_t - P^Y_{t - 1} + I^{Y^\prime}_{1t} \leq E^Y_{t - 1}\\
    0 &\leq P^X_t + M_{Pt} - P^Y_{t - 1} + I^{Y^\prime}_{1t} \leq E^Y_{t - 1}\\
    -P^X_t + P^Y_{t - 1} - I^{Y^\prime}_{1t} &\leq M_{Pt} \leq E^Y_{t - 1} - P^X_t + P^Y_{t - 1} - I^{Y^\prime}_{1t}.
\end{align*}
Hence
\begin{align*}
\Aboxed{M_{Pt} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-P^X_t + P^Y_{t - 1} - I^{Y^\prime}_{1t}, E^Y_{t - 1} - P^X_t + P^Y_{t - 1} - I^{Y^\prime}_{1t}\right)}\\
    \Aboxed{P^Y_t &= P^X_t + M_{Pt}}\\
    \Aboxed{P^{Y^\prime}_t &= P^X_t + M_{Pt} - P^Y_{t - 1} + I^{Y^\prime}_{1t}}
\end{align*}

For $R^Y_{At}$ we can consider:
\begin{equation*}
    R^Y_{At} = R^Y_{A(t - 1)} + R^{Y^\prime}_{At}
\end{equation*}
and we place MD on the \textit{incidence} as
\begin{equation*}
    R^{Y^\prime}_{At} = R^{X^\prime}_{At} + M^\prime_{R_At}.
\end{equation*}
We can derive bounds as:
\begin{align*}
    0 &\leq R^{Y^\prime}_{At} \leq A^Y_{t - 1}\\
    0 &\leq R^{X^\prime}_{At} + M^\prime_{At} \leq A^Y_{t - 1}\\
    -R^{X^\prime}_{At} &\leq  M^\prime_{At} \leq A^Y_{t - 1} - R^{X^\prime}_{At}
\end{align*}
and thus:
\begin{align*}
\Aboxed{M_{R_At} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-R^{X^\prime}_{At}, A^Y_{t - 1} - R^{X^\prime}_{At}\right)}\\
    \Aboxed{R^{Y^\prime}_{At} &= R^{X^\prime}_{At} + M^\prime_{R_At}}\\
    \Aboxed{R^Y_{At} &= R^X_{At} + M^\prime_{R_At}}
\end{align*}

For $A^Y_t$ we can consider that
\begin{equation*}
    A^Y_t = A^Y_{t - 1} + A^{Y^\prime}_t - R^{Y^\prime}_{At}
\end{equation*}
and place MD on the \textit{counts}, such that
\begin{equation*}
    A^Y_t = A^X_t + M_{At}
\end{equation*}
Since $0 \leq A^{Y^\prime}_t \leq E^Y_{t - 1} - P^{Y^\prime}_t$, we can derive bounds:
\begin{align*}
    0 &\leq A^{Y^\prime}_t \leq E^Y_{t - 1} - P^{Y^\prime}_t\\
    0 &\leq A^Y_t - A^Y_{t - 1} + R^{Y^\prime}_{At} \leq E^Y_{t - 1} - P^{Y^\prime}_t\\
    0 &\leq A^X_t + M_{At} - A^Y_{t - 1} + R^{Y^\prime}_{At} \leq E^Y_{t - 1} - P^{Y^\prime}_t\\
    -A^X_t + A^Y_{t - 1} - R^{Y^\prime}_{At} &\leq M_{At} \leq E^Y_{t - 1} - P^{Y^\prime}_t - A^X_t + A^Y_{t - 1} - R^{Y^\prime}_{At}
\end{align*}
Hence
\begin{align*}
\Aboxed{M_{At} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-A^X_t + A^Y_{t - 1} - R^{Y^\prime}_{At}, E^Y_{t - 1} - P^{Y^\prime}_t - A^X_t + A^Y_{t - 1} - R^{Y^\prime}_{At}\right)}\\
    \Aboxed{A^Y_t &= A^X_t + M_{At}}\\
    \Aboxed{A^{Y^\prime}_t &= A^X_t + M_{At} - A^Y_{t - 1} + R^{Y^\prime}_{At}}
\end{align*}

Then, we have that
\begin{equation*}
    E^Y_t = E^Y_{t - 1} + E^{Y^\prime}_t - P^{Y^\prime}_t - A^{Y^\prime}_t
\end{equation*}
and we place MD on the \textit{count}, such that
\begin{equation*}
    E^Y_t = E^X_t + M_{Et}
\end{equation*}
Since $0 \leq E^{Y^\prime}_t \leq S^Y_{t - 1}$ we can derive bounds:
\begin{align*}
    0 &\leq E^{Y^\prime}_t \leq S^Y_{t - 1}\\
    0 &\leq E^Y_t - E^Y_{t - 1} + P^{Y^\prime}_t + A^{Y^\prime}_t \leq S^Y_{t - 1}\\
    0 &\leq E^X_t + M_{Et} - E^Y_{t - 1} + P^{Y^\prime}_t + A^{Y^\prime}_t \leq S^Y_{t - 1}\\
    -E^X_t + E^Y_{t - 1} - P^{Y^\prime}_t - A^{Y^\prime}_t &\leq M_{Et} \leq S^Y_{t - 1} - E^X_t + E^Y_{t - 1} - P^{Y^\prime}_t - A^{Y^\prime}_t 
\end{align*}
and hence
\begin{align*}
\Aboxed{M_{Et} &\sim \mbox{Skellam}\left(\lambda_1, \lambda_2\right)I\left(-E^X_t + E^Y_{t - 1} - P^{Y^\prime}_t - A^{Y^\prime}_t, S^Y_{t - 1} - E^X_t + E^Y_{t - 1} - P^{Y^\prime}_t - A^{Y^\prime}_t\right)}\\
    \Aboxed{E^Y_t &= E^X_t + M_{Et}}\\
    \Aboxed{E^{Y^\prime}_t &= E^X_t + M_{Et} - E^Y_{t - 1} + P^{Y^\prime}_t + A^{Y^\prime}_t}
\end{align*}

Finally we have
\begin{align*}
    \Aboxed{S^Y_t &= S^Y_{t - 1} - E^{Y^\prime}_t}
\end{align*}

\section*{Some maths on DA in older notation}

We have observed data $\by$ and hidden states $\bx$ with parameters $\btheta$. If we assume that the parameters are fixed, then iFFBS~\citep{touloupouetal:2020} targets:
\begin{equation}
    \pi\left(\bx \mid \by\right) \propto \pi\left(\by \mid \bx\right)\pi\left(\bx\right). \label{eq:iFFBS}
\end{equation}
Currently $\pi\left(\by \mid \bx\right)$ is deterministic, but could be made stochastic (e.g. Poisson / NB). My understanding from e.g. \cite{lahoz_schneider:2014} is that classical data assimilation takes a forecast $\bx$ and updates it given the data to give an adjusted forecast $\bx^a$ such that:
\begin{equation}
    \pi\left(\bx^a \mid \by, \bx\right) \propto \pi\left(\by \mid \bx^a\right)\pi\left(\bx^a \mid \bx\right), \label{eq:DA}
\end{equation}
where $\bx$ has p.d.f. $\pi\left(\bx\right)$ as per equation (\ref{eq:iFFBS}). Here $\pi\left(\bx^a \mid \bx\right)$ is some kind of model discrepancy term and $\pi\left(\by \mid \bx^a\right)$ is some kind of observation error. 

Hence the two can be made equivalent (at least in terms of the joint distribution of $\bx$ and $\bx^a$) if we targeted:
\begin{equation}
    \pi\left(\bx^a, \bx \mid \by\right) \propto \pi\left(\by \mid \bx^a\right)\pi\left(\bx^a \mid \bx\right)\pi\left(\bx\right). \label{eq:joint}
\end{equation}
This could be estimated using iFFBS given functional forms for $\pi\left(\by \mid \bx^a\right)$ and $\pi\left(\bx^a \mid \bx\right)$. This of course updates $\bx$ as well as $\bx^a$ in light of the data $\by$ (this targets the full Bayesian posterior and not the conditional as in (\ref{eq:DA})).

\section{Time-series}
Let $\pi\left(x_t \mid x_{t-1}\right)$ correspond to the p.d.f. for the propagation of (hidden) states at time $t$ given the states at $t-1$ defined by the simulation model (ignoring the parameters for brevity). Thus:
\begin{equation}
    \pi\left(\by, \bx\right) = \pi\left(x_0\right)\prod_{t=1}^T \pi\left(y_t \mid x_t\right)\pi\left(x_t \mid x_{t - 1}\right).
\end{equation}
If we don't know $\bx$, then we have to marginalise over the unknown $\bx$, which is what the iFFBS does using MCMC.

I think the DA version would be:
\begin{equation}
    \pi\left(\by, \bx^a, \bx\right) = \pi\left(x_0\right)\pi\left(x_0^a \mid x^0\right)\prod_{t=1}^T \pi\left(y_t \mid x_t^a\right)\pi\left(x_t^a \mid x_t\right)\pi\left(x_t \mid x^a_{t - 1}\right).
\end{equation}

\bibliography{/home/tj/Documents/ref}

\end{document}
